{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정보 크롤링 (pdf 링크 / 날짜 / 제목 / 증권사이름 / pdf 파일이름 / txt 파일이름)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def crawl_info():\n",
    "    total_info = []\n",
    "    page = 1  # 페이지 초기값 설정\n",
    "    while True:\n",
    "        URL = f'https://finance.naver.com/research/debenture_list.naver?keyword=&brokerCode=&searchType=writeDate&writeFromDate=2014-01-01&writeToDate=2023-12-31&x=46&y=21&page={page}'\n",
    "        response = requests.get(URL)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table_tags = soup.select('tr')\n",
    "        info = []\n",
    "        for table_tag in table_tags:\n",
    "            try:\n",
    "                td_tags = table_tag.select('td')\n",
    "                if len(td_tags) >= 2:\n",
    "                    title_tag = td_tags[0].select_one('a')\n",
    "                    pdf_tags = table_tag.select('td.file > a')\n",
    "                    date_tags = table_tag.select('td.date')\n",
    "                    broker_name = td_tags[1].text.strip()  # 증권사 이름 가져오기\n",
    "                    if pdf_tags and date_tags:\n",
    "                        pdf = pdf_tags[0].attrs['href']\n",
    "                        date = date_tags[0].text\n",
    "                        title = title_tag.text\n",
    "                        pattern = '\\\\d+\\\\.pdf'\n",
    "                        file_name = re.findall(pattern, pdf)\n",
    "                        file_name = date + '_' + file_name[0]\n",
    "                        info.append((pdf, date, title, broker_name, file_name))\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "        total_info.extend(info)\n",
    "        \n",
    "        # \"맨뒤\" 버튼이 없을 경우 탐색을 멈춤\n",
    "        last_button = soup.select_one('td.pgRR > a')\n",
    "        if not last_button:\n",
    "            break\n",
    "        \n",
    "        page += 1  # 다음 페이지로 이동\n",
    "\n",
    "    # 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(total_info, columns=[\"pdf_link\", \"date\", \"title\", \"broker_name\", \"file_name\"])\n",
    "    df['content_file'] = df['file_name'].apply(lambda x: x.replace('.pdf', '.txt'))\n",
    "\n",
    "    # 데이터프레임을 CSV 파일로 저장\n",
    "    df.to_csv(\"pdf_link_crawl_add_txt.csv\", sep='\\t', index=False)\n",
    "    return 'pdf_link_crawl_add_txt.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdf_link_crawl_add_txt.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "def pdf_downloader(csv_file_name):\n",
    "\n",
    "    # CSV 파일을 불러와서 데이터프레임으로 변환\n",
    "    df = pd.read_csv(csv_file_name, sep='\\t')\n",
    "\n",
    "    # 데이터프레임을 리스트 안의 튜플 데이터 형태로 변환\n",
    "    total_info = [tuple(row) for row in df.values]\n",
    "\n",
    "    dir = './reportpdf/'\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    for i in range(len(total_info)):\n",
    "        if i==0:\n",
    "            print('pdf 다운로드 시작')\n",
    "        try:\n",
    "            pattern='\\\\d+\\\\.pdf'\n",
    "            file_name = re.findall(pattern, total_info[i][0])\n",
    "            file_name = total_info[i][1]+'_'+file_name[0]\n",
    "            urllib.request.urlretrieve(total_info[i][0], dir+file_name)\n",
    "        except:\n",
    "            print(f\"error : {total_info[i][0]}\")\n",
    "            print(traceback.format_exc())\n",
    "            i += 1 # 에러가 발생한 경우, 해당 항목부터 다시 크롤링할 수 있도록 i를 1 증가시킴\n",
    "        if i!=0 and i%100==0:\n",
    "            print(f'pdf 다운로드 진행률 : {i}/{len(total_info)}') # 100개 다운로드마다 알림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf 다운로드 시작\n",
      "pdf 다운로드 진행률 : 100/4898\n",
      "pdf 다운로드 진행률 : 200/4898\n",
      "pdf 다운로드 진행률 : 300/4898\n",
      "pdf 다운로드 진행률 : 400/4898\n",
      "pdf 다운로드 진행률 : 500/4898\n",
      "pdf 다운로드 진행률 : 600/4898\n",
      "pdf 다운로드 진행률 : 700/4898\n",
      "pdf 다운로드 진행률 : 800/4898\n",
      "pdf 다운로드 진행률 : 900/4898\n",
      "error : https://ssl.pstatic.net/imgstock/upload/research/debenture/1688340332432.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SDA05\\AppData\\Local\\Temp\\ipykernel_10676\\3619605239.py\", line 26, in pdf_downloader\n",
      "    urllib.request.urlretrieve(total_info[i][0], dir+file_name)\n",
      "  File \"c:\\Users\\SDA05\\anaconda3\\envs\\mecab\\lib\\urllib\\request.py\", line 278, in urlretrieve\n",
      "    raise ContentTooShortError(\n",
      "urllib.error.ContentTooShortError: <urlopen error retrieval incomplete: got only 265686 out of 466542 bytes>\n",
      "\n",
      "pdf 다운로드 진행률 : 1000/4898\n",
      "pdf 다운로드 진행률 : 1100/4898\n",
      "pdf 다운로드 진행률 : 1200/4898\n",
      "pdf 다운로드 진행률 : 1300/4898\n",
      "pdf 다운로드 진행률 : 1400/4898\n",
      "error : https://ssl.pstatic.net/imgstock/upload/research/debenture/1681692957681.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SDA05\\AppData\\Local\\Temp\\ipykernel_10676\\3619605239.py\", line 26, in pdf_downloader\n",
      "    urllib.request.urlretrieve(total_info[i][0], dir+file_name)\n",
      "  File \"c:\\Users\\SDA05\\anaconda3\\envs\\mecab\\lib\\urllib\\request.py\", line 278, in urlretrieve\n",
      "    raise ContentTooShortError(\n",
      "urllib.error.ContentTooShortError: <urlopen error retrieval incomplete: got only 266288 out of 722318 bytes>\n",
      "\n",
      "pdf 다운로드 진행률 : 1500/4898\n",
      "pdf 다운로드 진행률 : 1600/4898\n",
      "pdf 다운로드 진행률 : 1700/4898\n",
      "pdf 다운로드 진행률 : 1800/4898\n",
      "pdf 다운로드 진행률 : 1900/4898\n",
      "pdf 다운로드 진행률 : 2000/4898\n",
      "pdf 다운로드 진행률 : 2100/4898\n",
      "pdf 다운로드 진행률 : 2200/4898\n",
      "pdf 다운로드 진행률 : 2300/4898\n",
      "pdf 다운로드 진행률 : 2400/4898\n",
      "pdf 다운로드 진행률 : 2500/4898\n",
      "pdf 다운로드 진행률 : 2600/4898\n",
      "pdf 다운로드 진행률 : 2700/4898\n",
      "pdf 다운로드 진행률 : 2800/4898\n",
      "pdf 다운로드 진행률 : 2900/4898\n",
      "pdf 다운로드 진행률 : 3000/4898\n",
      "pdf 다운로드 진행률 : 3100/4898\n",
      "pdf 다운로드 진행률 : 3200/4898\n",
      "pdf 다운로드 진행률 : 3300/4898\n",
      "pdf 다운로드 진행률 : 3400/4898\n",
      "pdf 다운로드 진행률 : 3500/4898\n",
      "pdf 다운로드 진행률 : 3600/4898\n",
      "pdf 다운로드 진행률 : 3700/4898\n",
      "pdf 다운로드 진행률 : 3800/4898\n",
      "pdf 다운로드 진행률 : 3900/4898\n",
      "pdf 다운로드 진행률 : 4000/4898\n",
      "pdf 다운로드 진행률 : 4100/4898\n",
      "pdf 다운로드 진행률 : 4200/4898\n",
      "pdf 다운로드 진행률 : 4300/4898\n",
      "pdf 다운로드 진행률 : 4400/4898\n",
      "pdf 다운로드 진행률 : 4500/4898\n",
      "pdf 다운로드 진행률 : 4600/4898\n",
      "pdf 다운로드 진행률 : 4700/4898\n",
      "pdf 다운로드 진행률 : 4800/4898\n"
     ]
    }
   ],
   "source": [
    "pdf_downloader('pdf_link_crawl_add_txt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러 발생한 pdf의 경우, 따로 다운로드하여 폴더에 저장함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
