{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정보 크롤링 (pdf 링크)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def crawl_info():\n",
    "    total_info = []\n",
    "    page = 1  # 페이지 초기값 설정\n",
    "    while True:\n",
    "        URL = f'https://finance.naver.com/research/debenture_list.naver?keyword=&brokerCode=&searchType=writeDate&writeFromDate=2014-01-01&writeToDate=2023-12-31&x=46&y=21&page={page}'\n",
    "        response = requests.get(URL)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table_tags = soup.select('tr')\n",
    "        info = []\n",
    "        for table_tag in table_tags:\n",
    "            try:\n",
    "                title_tag = table_tag.select_one('td > a')\n",
    "                pdf_tags = table_tag.select('td.file > a')\n",
    "                date_tags = table_tag.select('td.date')\n",
    "                if pdf_tags!=[] and date_tags!=[]:\n",
    "                    pdf = pdf_tags[0].attrs['href']\n",
    "                    date = date_tags[0].text\n",
    "                    title = title_tag.text\n",
    "                    pattern='\\\\d+\\\\.pdf'\n",
    "                    file_name = re.findall(pattern, pdf)\n",
    "                    file_name = date+'_'+file_name[0]\n",
    "                    info.append((pdf, date, title, file_name))\n",
    "            except:\n",
    "                title_tag = table_tag.select_one('td > a')\n",
    "                print(f'error {title_tag.text}')\n",
    "        total_info.extend(info)\n",
    "        \n",
    "        # \"맨뒤\" 버튼이 없을 경우 탐색을 멈춤\n",
    "        last_button = soup.select_one('td.pgRR > a')\n",
    "        if not last_button:\n",
    "            break\n",
    "        \n",
    "        page += 1  # 다음 페이지로 이동\n",
    "\n",
    "    # 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(total_info, columns=[\"pdf_link\", \"date\", \"title\", \"file_name\"])\n",
    "    df['content_file'] = df['file_name'].apply(lambda x : x.replace('.pdf', '.txt'))\n",
    "\n",
    "    # 데이터프레임을 CSV 파일로 저장\n",
    "    df.to_csv(\"pdf_link_crawl_add_txt.csv\", sep='\\t', index=False)\n",
    "    return 'pdf_link_crawl_add_txt.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdf_link_crawl_add_txt.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "def pdf_downloader(csv_file_name):\n",
    "\n",
    "    # CSV 파일을 불러와서 데이터프레임으로 변환\n",
    "    df = pd.read_csv(csv_file_name, sep='\\t')\n",
    "\n",
    "    # 데이터프레임을 리스트 안의 튜플 데이터 형태로 변환\n",
    "    total_info = [tuple(row) for row in df.values]\n",
    "\n",
    "    dir = './reportpdf/'\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    for i in range(len(total_info)):\n",
    "        if i==0:\n",
    "            print('pdf 다운로드 시작')\n",
    "        try:\n",
    "            pattern='\\\\d+\\\\.pdf'\n",
    "            file_name = re.findall(pattern, total_info[i][0])\n",
    "            file_name = total_info[i][1]+'_'+file_name[0]\n",
    "            urllib.request.urlretrieve(total_info[i][0], dir+file_name)\n",
    "        except:\n",
    "            print(f\"error : {total_info[i][0]}\")\n",
    "            print(traceback.format_exc())\n",
    "            i += 1 # 에러가 발생한 경우, 해당 항목부터 다시 크롤링할 수 있도록 i를 1 증가시킴\n",
    "        if i!=0 and i%100==0:\n",
    "            print(f'pdf 다운로드 진행률 : {i}/{len(total_info)}') # 100개 다운로드마다 알림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf 다운로드 시작\n",
      "pdf 다운로드 진행률 : 100/4898\n",
      "pdf 다운로드 진행률 : 200/4898\n",
      "pdf 다운로드 진행률 : 300/4898\n",
      "pdf 다운로드 진행률 : 400/4898\n",
      "pdf 다운로드 진행률 : 500/4898\n",
      "pdf 다운로드 진행률 : 600/4898\n",
      "pdf 다운로드 진행률 : 700/4898\n",
      "pdf 다운로드 진행률 : 800/4898\n",
      "pdf 다운로드 진행률 : 900/4898\n",
      "error : https://ssl.pstatic.net/imgstock/upload/research/debenture/1688340332432.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SDA05\\AppData\\Local\\Temp\\ipykernel_10676\\3619605239.py\", line 26, in pdf_downloader\n",
      "    urllib.request.urlretrieve(total_info[i][0], dir+file_name)\n",
      "  File \"c:\\Users\\SDA05\\anaconda3\\envs\\mecab\\lib\\urllib\\request.py\", line 278, in urlretrieve\n",
      "    raise ContentTooShortError(\n",
      "urllib.error.ContentTooShortError: <urlopen error retrieval incomplete: got only 265686 out of 466542 bytes>\n",
      "\n",
      "pdf 다운로드 진행률 : 1000/4898\n",
      "pdf 다운로드 진행률 : 1100/4898\n",
      "pdf 다운로드 진행률 : 1200/4898\n",
      "pdf 다운로드 진행률 : 1300/4898\n",
      "pdf 다운로드 진행률 : 1400/4898\n",
      "error : https://ssl.pstatic.net/imgstock/upload/research/debenture/1681692957681.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SDA05\\AppData\\Local\\Temp\\ipykernel_10676\\3619605239.py\", line 26, in pdf_downloader\n",
      "    urllib.request.urlretrieve(total_info[i][0], dir+file_name)\n",
      "  File \"c:\\Users\\SDA05\\anaconda3\\envs\\mecab\\lib\\urllib\\request.py\", line 278, in urlretrieve\n",
      "    raise ContentTooShortError(\n",
      "urllib.error.ContentTooShortError: <urlopen error retrieval incomplete: got only 266288 out of 722318 bytes>\n",
      "\n",
      "pdf 다운로드 진행률 : 1500/4898\n",
      "pdf 다운로드 진행률 : 1600/4898\n",
      "pdf 다운로드 진행률 : 1700/4898\n",
      "pdf 다운로드 진행률 : 1800/4898\n",
      "pdf 다운로드 진행률 : 1900/4898\n",
      "pdf 다운로드 진행률 : 2000/4898\n",
      "pdf 다운로드 진행률 : 2100/4898\n",
      "pdf 다운로드 진행률 : 2200/4898\n",
      "pdf 다운로드 진행률 : 2300/4898\n",
      "pdf 다운로드 진행률 : 2400/4898\n",
      "pdf 다운로드 진행률 : 2500/4898\n",
      "pdf 다운로드 진행률 : 2600/4898\n",
      "pdf 다운로드 진행률 : 2700/4898\n",
      "pdf 다운로드 진행률 : 2800/4898\n",
      "pdf 다운로드 진행률 : 2900/4898\n",
      "pdf 다운로드 진행률 : 3000/4898\n",
      "pdf 다운로드 진행률 : 3100/4898\n",
      "pdf 다운로드 진행률 : 3200/4898\n",
      "pdf 다운로드 진행률 : 3300/4898\n",
      "pdf 다운로드 진행률 : 3400/4898\n",
      "pdf 다운로드 진행률 : 3500/4898\n",
      "pdf 다운로드 진행률 : 3600/4898\n",
      "pdf 다운로드 진행률 : 3700/4898\n",
      "pdf 다운로드 진행률 : 3800/4898\n",
      "pdf 다운로드 진행률 : 3900/4898\n",
      "pdf 다운로드 진행률 : 4000/4898\n",
      "pdf 다운로드 진행률 : 4100/4898\n",
      "pdf 다운로드 진행률 : 4200/4898\n",
      "pdf 다운로드 진행률 : 4300/4898\n",
      "pdf 다운로드 진행률 : 4400/4898\n",
      "pdf 다운로드 진행률 : 4500/4898\n",
      "pdf 다운로드 진행률 : 4600/4898\n",
      "pdf 다운로드 진행률 : 4700/4898\n",
      "pdf 다운로드 진행률 : 4800/4898\n"
     ]
    }
   ],
   "source": [
    "pdf_downloader('pdf_link_crawl_add_txt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러 발생한 pdf의 경우, 따로 다운로드하여 폴더에 저장함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf to txt (clensing 후 저장) - 수정 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "def pdf_to_txt(csv_file_name):\n",
    "    # CSV 파일을 불러와서 데이터프레임으로 변환\n",
    "    df = pd.read_csv(csv_file_name, sep='\\t')\n",
    "\n",
    "    dir = './reportpdf/'\n",
    "    dir2 = './reporttxt/'\n",
    "    if not os.path.exists(dir2):\n",
    "        os.makedirs(dir2)\n",
    "\n",
    "    if i==0:\n",
    "        print('텍스트 파일로 변환 시작')\n",
    "        \n",
    "    for i in range(len(df)):\n",
    "        df_info_list = df.iloc[i].to_list()\n",
    "        pdf_name = dir + df_info_list[3]\n",
    "        \n",
    "        with fitz.open(pdf_name) as doc:\n",
    "            txt_content = ''\n",
    "            for page in doc:\n",
    "                txt_content += page.get_text() + '\\n'  # 각 페이지의 텍스트를 가져와서 줄 바꿈을 추가하여 문장 단위로 구분\n",
    "            txt_name = dir2 + df_info_list[4]\n",
    "            with open(txt_name, 'w', encoding='utf-8') as f:\n",
    "                f.write(txt_content)\n",
    "        \n",
    "        if i!=0 and i % 100 == 0:\n",
    "            print(f'txt {i}개 변환 완료')  # 100개 변환마다 알림\n",
    "    \n",
    "    return '모든 pdf 파일에 대해 txt 파일로 변환 완료'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_to_txt(\"pdf_link_crawl_add_txt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
